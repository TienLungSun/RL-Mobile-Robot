{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: RL-Adventure https://github.com/higgsfield/RL-Adventure-2/blob/master/3.ppo.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "if(torch.cuda.is_available()):\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(device, torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device= torch.device(\"cpu\")\n",
    "    print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define NN for PPO AC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.normal_(m.weight, mean=0., std=0.1)\n",
    "        nn.init.constant_(m.bias, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions import Normal\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs, hidden_size1, hidden_size2, std=0.0):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        \n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(num_inputs, hidden_size1),\n",
    "            nn.LayerNorm(hidden_size1),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size1, hidden_size2),\n",
    "            nn.LayerNorm(hidden_size2),\n",
    "            nn.Tanh(),    \n",
    "            nn.Linear(hidden_size2, 1),\n",
    "        )\n",
    "        \n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(num_inputs, hidden_size1),\n",
    "            nn.LayerNorm(hidden_size1),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size1, hidden_size2),\n",
    "            nn.LayerNorm(hidden_size2),\n",
    "            nn.Tanh(),    \n",
    "            nn.Linear(hidden_size2, num_outputs),\n",
    "            nn.Tanh()   \n",
    "        )\n",
    "        self.log_std = nn.Parameter(torch.ones(1, num_outputs) * std)\n",
    "        \n",
    "        self.apply(init_weights)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        value = self.critic(x)\n",
    "        mu    = self.actor(x)\n",
    "        std   = self.log_std.exp().expand_as(mu)\n",
    "        dist  = Normal(mu, std)\n",
    "        return dist, value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gae(next_value, rewards, masks, values, gamma=0.99, tau=0.95):\n",
    "    values = values + [next_value]\n",
    "    gae = 0\n",
    "    returns = []\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        delta = rewards[step] + gamma * values[step + 1] * masks[step] - values[step]\n",
    "        gae = delta + gamma * tau * masks[step] * gae\n",
    "        returns.insert(0, gae + values[step])\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "def ppo_iter(mini_batch_size, states, actions, log_probs, returns, advantage):\n",
    "    batch_size = states.size(0)\n",
    "    for _ in range(batch_size // mini_batch_size):\n",
    "        rand_ids = np.random.randint(0, batch_size, mini_batch_size)\n",
    "        yield states[rand_ids, :], actions[rand_ids, :], log_probs[rand_ids, :], returns[rand_ids, :], advantage[rand_ids, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppo_update(ppo_epochs, mini_batch_size, states, actions, log_probs, returns, advantages, clip_param=0.2):\n",
    "    for _ in range(ppo_epochs):\n",
    "        for state, action, old_log_probs, return_, advantage in ppo_iter(mini_batch_size, states, actions, log_probs, returns, advantages):\n",
    "            dist, value = model(state)\n",
    "            entropy = dist.entropy().mean()\n",
    "            new_log_probs = dist.log_prob(action)\n",
    "\n",
    "            ratio = (new_log_probs - old_log_probs).exp()\n",
    "            surr1 = ratio * advantage\n",
    "            surr2 = torch.clamp(ratio, 1.0 - clip_param, 1.0 + clip_param) * advantage\n",
    "\n",
    "            actor_loss  = - torch.min(surr1, surr2).mean()\n",
    "            critic_loss = (return_ - value).pow(2).mean()\n",
    "\n",
    "            loss = 0.5 * critic_loss + actor_loss - 0.001 * entropy\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    return float(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test NN performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_env(Max_test_frames=100):\n",
    "    env.reset()\n",
    "    step_result = env.get_steps(behaviorName) \n",
    "    DecisionSteps = step_result[0] \n",
    "    state = DecisionSteps.obs[0]        \n",
    "    total_reward = 0\n",
    "    frame_count = 0\n",
    "    stop_test = False\n",
    "    while (not stop_test):  \n",
    "        frame_count = frame_count + 1\n",
    "        if(frame_count > Max_test_frames):\n",
    "            stop_test = True\n",
    "        else:\n",
    "            state = torch.FloatTensor(state).to(device)\n",
    "            dist, _ = model(state)\n",
    "            action = dist.sample()\n",
    "            env.set_actions(behaviorName, np.array(action.cpu()))\n",
    "            env.step()\n",
    "            step_result = env.get_steps(behaviorName) \n",
    "            DecisionSteps = step_result[0]\n",
    "            TerminalSteps = step_result[1]\n",
    "            if(len(TerminalSteps) >0): # if reach terminal step, then stop\n",
    "                reward = TerminalSteps.reward\n",
    "                total_reward += reward\n",
    "                stop_test = True\n",
    "            else:\n",
    "                next_state = DecisionSteps.obs[0]\n",
    "                reward = DecisionSteps.reward\n",
    "                total_reward += reward\n",
    "                state = next_state\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect to Unity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlagents_envs.environment import UnityEnvironment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 確認 Unity CarAgent 的 Behaivor Type = Default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = UnityEnvironment(file_name= None, base_port=5004) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instantiate AC PPO NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My Behavior?team=0 \n",
      " BehaviorSpec(observation_shapes=[(19,)], action_type=<ActionType.CONTINUOUS: 1>, action_shape=2)\n"
     ]
    }
   ],
   "source": [
    "behaviorNames = list(env.behavior_specs.keys())\n",
    "behaviorName = behaviorNames[0]\n",
    "behavior_spec = env.behavior_specs[behaviorName]\n",
    "print(behaviorName, \"\\n\", behavior_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19 2\n"
     ]
    }
   ],
   "source": [
    "num_inputs  = behavior_spec.observation_shapes[0][0]\n",
    "num_outputs = behavior_spec.action_shape\n",
    "print(num_inputs, num_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NN parameters\n",
    "hidden_size1      =128\n",
    "hidden_size2      = 64\n",
    "lr               = 3e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "model = ActorCritic(num_inputs, num_outputs, hidden_size1, hidden_size2).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 訓練前先啟動 tensor board server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current directory is C:\\Users\\admin\\Google 雲端硬碟\\0. 教學 IE 351, 562 VR1 Unity 3D\n"
     ]
    }
   ],
   "source": [
    "# 此行顯示目前這個 ipython notebook 的目錄\n",
    "!echo The current directory is %CD%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在 python terminal window, 到這個 ipython notebook 的目錄下輸入:\n",
    "# tensorboard --logdir=runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 網頁 localhost:6006 連到 TB server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NN training params:\n",
    "num_steps        = 20  \n",
    "mini_batch_size  = 5\n",
    "ppo_epochs       = 4\n",
    "threshold_reward = 9.98 #9.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPO update, loss = 13.60\n",
      "PPO update, loss = 2.98\n",
      "PPO update, loss = 2.53\n",
      "PPO update, loss = 6.37\n",
      "Frame  100 , test NN, Avg reward = 9.95 \n",
      "PPO update, loss = 1.92\n",
      "PPO update, loss = 2.75\n",
      "PPO update, loss = 1.77\n",
      "PPO update, loss = 2.04\n",
      "PPO update, loss = 0.73\n",
      "Frame  200 , test NN, Avg reward = 9.95 \n",
      "PPO update, loss = 0.35\n",
      "PPO update, loss = 2.19\n",
      "PPO update, loss = 2.03\n",
      "PPO update, loss = 0.72\n",
      "PPO update, loss = 0.94\n",
      "Frame  300 , test NN, Avg reward = 9.95 \n",
      "PPO update, loss = 0.88\n",
      "PPO update, loss = 0.13\n",
      "PPO update, loss = 0.16\n",
      "PPO update, loss = 0.05\n",
      "PPO update, loss = 0.29\n",
      "Frame  400 , test NN, Avg reward = 9.96 \n",
      "PPO update, loss = 0.08\n",
      "PPO update, loss = 0.10\n",
      "PPO update, loss = -0.43\n",
      "PPO update, loss = -0.40\n",
      "PPO update, loss = -0.06\n",
      "Frame  500 , test NN, Avg reward = 9.97 \n",
      "PPO update, loss = -0.47\n",
      "PPO update, loss = -0.62\n",
      "PPO update, loss = -0.46\n",
      "PPO update, loss = -0.05\n",
      "PPO update, loss = 0.04\n",
      "Frame  600 , test NN, Avg reward = 8.97 \n",
      "PPO update, loss = 0.03\n",
      "PPO update, loss = 2.98\n",
      "PPO update, loss = -0.58\n",
      "PPO update, loss = 0.16\n",
      "PPO update, loss = -0.07\n",
      "Frame  700 , test NN, Avg reward = 9.95 \n",
      "PPO update, loss = -0.28\n",
      "PPO update, loss = 0.16\n",
      "PPO update, loss = 0.41\n",
      "PPO update, loss = -0.33\n",
      "PPO update, loss = -0.42\n",
      "Frame  800 , test NN, Avg reward = 9.96 \n",
      "PPO update, loss = -0.37\n",
      "PPO update, loss = -0.17\n",
      "PPO update, loss = -0.13\n",
      "PPO update, loss = -0.04\n",
      "PPO update, loss = -0.19\n",
      "Frame  900 , test NN, Avg reward = 9.96 \n",
      "PPO update, loss = -0.29\n",
      "PPO update, loss = 0.50\n",
      "PPO update, loss = 0.09\n",
      "PPO update, loss = 0.40\n",
      "PPO update, loss = 0.13\n",
      "Frame  1000 , test NN, Avg reward = 9.47 \n",
      "PPO update, loss = -0.19\n",
      "PPO update, loss = 116.00\n",
      "PPO update, loss = 1.30\n",
      "PPO update, loss = 8.51\n",
      "PPO update, loss = 2.25\n",
      "Frame  1100 , test NN, Avg reward = 9.96 \n",
      "PPO update, loss = -0.45\n",
      "PPO update, loss = -1.13\n",
      "PPO update, loss = -0.06\n",
      "PPO update, loss = -0.26\n",
      "PPO update, loss = -0.68\n",
      "Frame  1200 , test NN, Avg reward = 9.96 \n",
      "PPO update, loss = -0.25\n",
      "PPO update, loss = -0.11\n",
      "PPO update, loss = -0.31\n",
      "PPO update, loss = -0.32\n",
      "PPO update, loss = -0.28\n",
      "Frame  1300 , test NN, Avg reward = 9.95 \n",
      "PPO update, loss = -0.23\n",
      "PPO update, loss = 0.04\n",
      "PPO update, loss = 0.39\n",
      "PPO update, loss = 0.03\n",
      "PPO update, loss = 0.20\n",
      "Frame  1400 , test NN, Avg reward = 9.97 \n",
      "PPO update, loss = 0.12\n",
      "PPO update, loss = -0.06\n",
      "PPO update, loss = 0.26\n",
      "PPO update, loss = 0.13\n",
      "PPO update, loss = -0.01\n",
      "Frame  1500 , test NN, Avg reward = 9.96 \n",
      "PPO update, loss = -0.06\n",
      "PPO update, loss = -0.03\n",
      "PPO update, loss = -0.26\n",
      "PPO update, loss = -0.17\n",
      "PPO update, loss = -0.23\n",
      "Frame  1600 , test NN, Avg reward = 9.96 \n",
      "PPO update, loss = -0.01\n",
      "PPO update, loss = -0.07\n",
      "PPO update, loss = 0.21\n",
      "PPO update, loss = 0.14\n",
      "PPO update, loss = -0.00\n",
      "Frame  1700 , test NN, Avg reward = 9.97 \n",
      "PPO update, loss = 0.34\n",
      "PPO update, loss = -0.22\n",
      "PPO update, loss = -0.15\n",
      "PPO update, loss = 0.22\n",
      "PPO update, loss = 0.24\n",
      "Frame  1800 , test NN, Avg reward = 9.96 \n",
      "PPO update, loss = -0.22\n",
      "PPO update, loss = 0.08\n",
      "PPO update, loss = -0.10\n",
      "PPO update, loss = 0.11\n",
      "PPO update, loss = -0.29\n",
      "Frame  1900 , test NN, Avg reward = 8.46 \n",
      "PPO update, loss = -0.04\n",
      "PPO update, loss = -0.01\n",
      "PPO update, loss = -0.13\n",
      "PPO update, loss = 0.12\n",
      "PPO update, loss = -0.04\n",
      "Frame  2000 , test NN, Avg reward = 9.96 \n",
      "PPO update, loss = 0.25\n",
      "PPO update, loss = 0.28\n",
      "PPO update, loss = -0.12\n",
      "PPO update, loss = -0.28\n",
      "PPO update, loss = 0.09\n",
      "Frame  2100 , test NN, Avg reward = 9.96 \n",
      "PPO update, loss = 0.14\n",
      "PPO update, loss = -0.11\n",
      "PPO update, loss = 0.04\n",
      "PPO update, loss = 0.12\n",
      "PPO update, loss = 0.10\n",
      "Frame  2200 , test NN, Avg reward = 9.96 \n",
      "PPO update, loss = 0.27\n",
      "PPO update, loss = -0.13\n",
      "PPO update, loss = 0.16\n",
      "PPO update, loss = -0.02\n",
      "PPO update, loss = -0.27\n",
      "Frame  2300 , test NN, Avg reward = 9.97 \n",
      "PPO update, loss = -0.01\n",
      "PPO update, loss = -0.14\n",
      "PPO update, loss = -0.01\n",
      "PPO update, loss = -0.06\n",
      "PPO update, loss = 0.07\n",
      "Frame  2400 , test NN, Avg reward = 9.97 \n",
      "PPO update, loss = 0.11\n",
      "PPO update, loss = -0.01\n",
      "PPO update, loss = -0.17\n",
      "PPO update, loss = 0.45\n",
      "PPO update, loss = -0.17\n",
      "Frame  2500 , test NN, Avg reward = 9.96 \n",
      "PPO update, loss = 0.23\n",
      "PPO update, loss = 0.03\n",
      "PPO update, loss = -0.09\n",
      "PPO update, loss = 0.44\n",
      "PPO update, loss = 0.09\n",
      "Frame  2600 , test NN, Avg reward = 9.95 \n",
      "PPO update, loss = 0.49\n",
      "PPO update, loss = 0.05\n",
      "PPO update, loss = -0.09\n",
      "PPO update, loss = 0.08\n",
      "PPO update, loss = -0.18\n",
      "Frame  2700 , test NN, Avg reward = 9.96 \n",
      "PPO update, loss = -0.01\n",
      "PPO update, loss = 0.09\n",
      "PPO update, loss = -0.12\n",
      "PPO update, loss = 0.03\n",
      "PPO update, loss = 0.07\n",
      "Frame  2800 , test NN, Avg reward = 9.96 \n",
      "PPO update, loss = 0.25\n",
      "PPO update, loss = -0.07\n",
      "PPO update, loss = -0.17\n",
      "PPO update, loss = 0.15\n",
      "PPO update, loss = 0.28\n",
      "Frame  2900 , test NN, Avg reward = 9.96 \n",
      "PPO update, loss = 0.48\n",
      "PPO update, loss = -0.01\n",
      "PPO update, loss = -0.08\n",
      "PPO update, loss = 2.56\n",
      "PPO update, loss = 0.01\n",
      "Frame  3000 , test NN, Avg reward = 9.96 \n",
      "PPO update, loss = -0.23\n",
      "PPO update, loss = -0.17\n",
      "PPO update, loss = 0.06\n",
      "PPO update, loss = -0.06\n",
      "PPO update, loss = 0.48\n",
      "Frame  3100 , test NN, Avg reward = 9.96 \n",
      "PPO update, loss = -0.14\n",
      "PPO update, loss = -0.30\n",
      "PPO update, loss = 0.10\n",
      "PPO update, loss = 0.21\n",
      "PPO update, loss = 0.05\n",
      "Frame  3200 , test NN, Avg reward = 9.96 \n",
      "PPO update, loss = -0.03\n",
      "PPO update, loss = -0.14\n",
      "PPO update, loss = -0.11\n",
      "PPO update, loss = 0.11\n",
      "PPO update, loss = 0.34\n",
      "Frame  3300 , test NN, Avg reward = 9.96 \n",
      "PPO update, loss = -0.31\n",
      "PPO update, loss = 0.26\n",
      "PPO update, loss = 0.44\n",
      "PPO update, loss = 0.09\n",
      "PPO update, loss = -0.06\n",
      "Frame  3400 , test NN, Avg reward = 9.97 \n",
      "PPO update, loss = 0.02\n",
      "PPO update, loss = 0.38\n",
      "PPO update, loss = -0.15\n",
      "PPO update, loss = -0.08\n",
      "PPO update, loss = 0.03\n",
      "Frame  3500 , test NN, Avg reward = 9.96 \n",
      "PPO update, loss = -0.16\n",
      "PPO update, loss = -0.15\n",
      "PPO update, loss = -0.03\n",
      "PPO update, loss = 4.77\n",
      "PPO update, loss = -0.28\n",
      "Frame  3600 , test NN, Avg reward = 9.96 \n",
      "PPO update, loss = -0.49\n",
      "PPO update, loss = -0.24\n",
      "PPO update, loss = -0.03\n",
      "PPO update, loss = 0.13\n",
      "PPO update, loss = 0.04\n",
      "Frame  3700 , test NN, Avg reward = 9.46 \n",
      "PPO update, loss = 0.18\n",
      "PPO update, loss = 3.12\n",
      "PPO update, loss = 5.21\n",
      "PPO update, loss = -0.62\n",
      "PPO update, loss = 0.57\n",
      "Frame  3800 , test NN, Avg reward = 8.47 \n",
      "PPO update, loss = 1.80\n",
      "PPO update, loss = -0.22\n",
      "PPO update, loss = -0.05\n",
      "PPO update, loss = -0.08\n",
      "PPO update, loss = -0.05\n",
      "Frame  3900 , test NN, Avg reward = 9.96 \n",
      "PPO update, loss = -0.26\n",
      "PPO update, loss = 0.03\n",
      "PPO update, loss = 0.20\n",
      "PPO update, loss = -0.13\n",
      "PPO update, loss = -0.05\n",
      "Frame  4000 , test NN, Avg reward = 9.96 \n",
      "PPO update, loss = -0.09\n",
      "PPO update, loss = -0.02\n",
      "PPO update, loss = -0.03\n",
      "PPO update, loss = -0.33\n",
      "PPO update, loss = -0.00\n",
      "Frame  4100 , test NN, Avg reward = 9.96 \n",
      "PPO update, loss = 0.24\n",
      "PPO update, loss = 0.17\n",
      "PPO update, loss = -0.11\n",
      "PPO update, loss = 0.01\n",
      "PPO update, loss = 0.77\n",
      "Frame  4200 , test NN, Avg reward = 9.96 \n",
      "PPO update, loss = -0.26\n",
      "PPO update, loss = -0.17\n",
      "PPO update, loss = 0.12\n",
      "PPO update, loss = -0.05\n",
      "PPO update, loss = 0.19\n",
      "Frame  4300 , test NN, Avg reward = 9.97 \n",
      "PPO update, loss = 0.26\n",
      "PPO update, loss = 0.07\n",
      "PPO update, loss = 0.02\n",
      "PPO update, loss = 0.28\n",
      "PPO update, loss = -0.22\n",
      "Frame  4400 , test NN, Avg reward = 9.95 \n",
      "PPO update, loss = 0.21\n",
      "PPO update, loss = -0.25\n",
      "PPO update, loss = -0.00\n",
      "PPO update, loss = -0.10\n",
      "PPO update, loss = 0.38\n",
      "Frame  4500 , test NN, Avg reward = 9.96 \n",
      "PPO update, loss = 13.46\n",
      "PPO update, loss = -1.95\n",
      "PPO update, loss = 2.72\n",
      "PPO update, loss = 0.04\n",
      "PPO update, loss = -0.69\n",
      "Frame  4600 , test NN, Avg reward = 9.97 \n",
      "PPO update, loss = 0.23\n",
      "PPO update, loss = 0.32\n",
      "PPO update, loss = 0.48\n",
      "PPO update, loss = 4.49\n",
      "PPO update, loss = -0.22\n",
      "Frame  4700 , test NN, Avg reward = 9.96 \n",
      "PPO update, loss = -0.50\n",
      "PPO update, loss = -0.51\n",
      "PPO update, loss = 0.08\n",
      "PPO update, loss = 0.26\n",
      "PPO update, loss = -0.00\n",
      "Frame  4800 , test NN, Avg reward = 9.95 \n",
      "PPO update, loss = -0.01\n",
      "PPO update, loss = -0.08\n",
      "PPO update, loss = -0.18\n",
      "PPO update, loss = 0.25\n",
      "PPO update, loss = 0.42\n",
      "Frame  4900 , test NN, Avg reward = 9.96 \n",
      "PPO update, loss = 0.14\n",
      "PPO update, loss = 0.11\n",
      "PPO update, loss = 0.12\n",
      "PPO update, loss = -0.04\n",
      "PPO update, loss = 0.01\n",
      "Frame  5000 , test NN, Avg reward = 9.95 \n",
      "PPO update, loss = -0.08\n"
     ]
    }
   ],
   "source": [
    "frame_idx  = 0\n",
    "max_frames = 5000   #15000\n",
    "env.reset()\n",
    "early_stop = False\n",
    "__printDetails = False\n",
    "\n",
    "while frame_idx < max_frames and not early_stop: #Run simulation max_frames steps\n",
    "    if(__printDetails):\n",
    "        print(\"Frame = \", frame_idx, end=\", \")\n",
    "    log_probs = []\n",
    "    values    = []\n",
    "    states    = []\n",
    "    actions   = []\n",
    "    rewards   = []\n",
    "    masks     = []\n",
    "    entropy = 0\n",
    "\n",
    "    step_result = env.get_steps(behaviorName) \n",
    "    DecisionSteps = step_result[0] \n",
    "    state = DecisionSteps.obs[0]\n",
    "    if(__printDetails):\n",
    "        print(\"step\", end = \":\")\n",
    "    for step in range(num_steps):\n",
    "        if(__printDetails and (step+1) % 5==0):\n",
    "            print(step+1, end = \", \")\n",
    "        state = torch.FloatTensor(state).to(device)\n",
    "        dist, value = model(state)\n",
    "\n",
    "        action = dist.sample()\n",
    "        if(int(torch.isnan(torch.min(action))) == 1) : #we have Nan when sampling distribution\n",
    "            print(\"Error: distribution=\", dist, \"%.2f, %.2f\" % (float(action[0][0]), float(action[0][1])))\n",
    "        env.set_actions(behaviorName, np.array(action.cpu()))\n",
    "        env.step()\n",
    "\n",
    "        step_result = env.get_steps(behaviorName) \n",
    "        DecisionSteps = step_result[0] \n",
    "        TerminalSteps = step_result[1]\n",
    "        if(len(TerminalSteps) >0): # if episode is termined, collect (s, r) from terminal step\n",
    "            next_state = TerminalSteps.obs[0]\n",
    "            reward = TerminalSteps.reward\n",
    "            if(__printDetails):\n",
    "                print(\"Reach goal, r= %.2f\" % reward)\n",
    "            mask=[0.0]\n",
    "        elif(len(DecisionSteps) >0): #otherwise collect (s, r) from decision steps\n",
    "            next_state = DecisionSteps.obs[0]\n",
    "            reward = DecisionSteps.reward\n",
    "            if(__printDetails and reward >= 5):\n",
    "                print(\"Hit obstacle!, r=\", reward)\n",
    "            mask=[1.0]\n",
    "\n",
    "        log_prob = dist.log_prob(action)\n",
    "        entropy += dist.entropy().mean()\n",
    "\n",
    "        log_probs.append(log_prob)\n",
    "        values.append(value)\n",
    "        rewards.append(torch.FloatTensor(reward).unsqueeze(1).to(device))\n",
    "        masks.append(torch.FloatTensor(mask).unsqueeze(1).to(device))\n",
    "\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        \n",
    "        frame_idx += 1\n",
    "        \n",
    "        if frame_idx % 100 == 0:  # Test NN performance every 100 frames\n",
    "            print(\"Frame \", frame_idx, \", test NN\", end=\", \")\n",
    "            \n",
    "            test_reward = np.mean([test_env(Max_test_frames=200) for _ in range(10)])\n",
    "            \n",
    "            print(\"Avg reward = %.2f \" % test_reward)\n",
    "            writer.add_scalar(\"Reward\", test_reward, frame_idx)\n",
    "            if (test_reward > threshold_reward):\n",
    "                early_stop = True\n",
    "                \n",
    "        if(len(TerminalSteps) >0): #if agent's episode is terminated, we need to get new episode state\n",
    "            if(__printDetails):\n",
    "                print(\"Run simulation one step to get initial state\")\n",
    "            step_result = env.get_steps(behaviorName) \n",
    "            DecisionSteps = step_result[0]\n",
    "            state = DecisionSteps.obs[0]\n",
    "        else:\n",
    "            state = next_state\n",
    "            \n",
    "    next_state = torch.FloatTensor(next_state).to(device)\n",
    "    _, next_value = model(next_state)\n",
    "    returns = compute_gae(next_value, rewards, masks, values)\n",
    "\n",
    "    returns   = torch.cat(returns).detach()\n",
    "    log_probs = torch.cat(log_probs).detach()\n",
    "    values    = torch.cat(values).detach()\n",
    "    states    = torch.cat(states)\n",
    "    actions   = torch.cat(actions)\n",
    "    advantage = returns - values\n",
    "    \n",
    "    loss = ppo_update(ppo_epochs, mini_batch_size, states, actions, log_probs, returns, advantage)\n",
    "    print(\"PPO update, loss = %.2f\" % loss)\n",
    "    writer.add_scalar(\"Loss\", loss, frame_idx)\n",
    "writer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 如果要中斷與 Unity 的連結:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 如果想要結束目前的 TB writter, 重起一個 TB writter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### env.close後如果需要重新 connect Unity 訓練可用下面捷徑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = UnityEnvironment(file_name= None, base_port=5004) \n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 如果訓練表現不好, 需要重新生一個 NN, 可用下面捷徑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ActorCritic(num_inputs, num_outputs, hidden_size1, hidden_size2).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 如果訓練效果不錯, 想要保留目前 NN (Save model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "datetime_dt = datetime.today()# 獲得當地時間\n",
    "datetime_str = datetime_dt.strftime(\"%Y_%m_%d_%H_%M_%S\")  # 格式化日期\n",
    "fname = datetime_str + \"_\" + str(frame_idx) + \".pkl\"\n",
    "torch.save(model.state_dict(), fname)\n",
    "print(\"Model is saved: \", fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 想要 load previously saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ActorCritic(num_inputs, num_outputs, hidden_size1, hidden_size2).to(device)\n",
    "model.load_state_dict(torch.load(\"2020_11_23_15_40_28_800.pkl\"))\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 看看目前 model 的控制表現 (Examine NN performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    r = test_env(Max_test_frames=200)\n",
    "    print(r, end =\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
