{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ref: RL-Adventure https://github.com/higgsfield/RL-Adventure-2/blob/master/3.ppo.ipynb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlagents_envs.environment import UnityEnvironment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "if(torch.cuda.is_available()):\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(device, torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device= torch.device(\"cpu\")\n",
    "    print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define NN for PPO AC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.normal_(m.weight, mean=0., std=0.1)\n",
    "        nn.init.constant_(m.bias, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs, hidden_size, std=0.0):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        \n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(num_inputs, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "        \n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(num_inputs, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, num_outputs),\n",
    "        )\n",
    "        self.log_std = nn.Parameter(torch.ones(1, num_outputs) * std)\n",
    "        \n",
    "        self.apply(init_weights)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        value = self.critic(x)\n",
    "        mu    = self.actor(x)\n",
    "        std   = self.log_std.exp().expand_as(mu)\n",
    "        dist  = Normal(mu, std)\n",
    "        return dist, value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect to Unity\n",
    "We handle a Unity scene that contains only one training world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = UnityEnvironment(file_name= None, base_port=5004) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 2\n"
     ]
    }
   ],
   "source": [
    "behaviorNames = list(env.behavior_specs.keys())\n",
    "behaviorName = behaviorNames[0]\n",
    "behavior_spec = env.behavior_specs[behaviorName]\n",
    "num_inputs  = behavior_spec.observation_shapes[0][0]\n",
    "num_outputs = behavior_spec.action_shape\n",
    "print(num_inputs, num_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instantiate a PPO-AC neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size      = 256\n",
    "lr               = 3e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ActorCritic(num_inputs, num_outputs, hidden_size).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# while frame_idx < max_frames and not early_stop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After N iterations, len(...) =N\n",
    "log_probs = []\n",
    "values    = []\n",
    "states    = [] # states[i].shape=[No. of Agents, state no.]\n",
    "actions   = [] # actions[i].shape = [No. of Agents, action no.]\n",
    "rewards   = [] # rewards[i].shape=[No. of Agents, 1]\n",
    "masks     = [] # masks[i].shape=[No. of Agents, 1]\n",
    "entropy = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# for _ in range(num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_result = env.get_steps(behaviorName) \n",
    "DecisionSteps = step_result[0] \n",
    "TerminalSteps = step_result[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4.2919607 -7.8069067 28.800442   1.7667229  1.9213159  2.4526217\n",
      "   4.         4.         4.         4.         3.4204714  3.111842 ]]\n"
     ]
    }
   ],
   "source": [
    "state = DecisionSteps.obs[0]\n",
    "print(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 12])\n"
     ]
    }
   ],
   "source": [
    "state = torch.FloatTensor(state).to(device)\n",
    "print(state.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal(loc: torch.Size([1, 2]), scale: torch.Size([1, 2]))\n",
      "tensor([[-1.3420]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "dist, value = model(state)\n",
    "print(dist)\n",
    "print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.5054, -1.2700]]) torch.Size([1, 2])\n"
     ]
    }
   ],
   "source": [
    "action = dist.sample()\n",
    "print(action, action.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.set_actions(behaviorName, np.array(action.cpu()))\n",
    "env.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 12)\n",
      "[-0.11507208]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "step_result = env.get_steps(behaviorName) \n",
    "DecisionSteps = step_result[0] \n",
    "TerminalSteps = step_result[1]\n",
    "\n",
    "if(len(TerminalSteps) >0): # if we have terminal step, collect (s, r) from terminal step\n",
    "    next_state = TerminalSteps.obs[0]\n",
    "    reward = TerminalSteps.reward\n",
    "    mask=0\n",
    "elif(len(DecisionSteps) >0): #otherwise collect (s, r) from decision steps\n",
    "    next_state = DecisionSteps.obs[0]\n",
    "    reward = DecisionSteps.reward\n",
    "    mask=1\n",
    "    \n",
    "print(next_state.shape)\n",
    "print(reward)\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.3377, -5.2006]], grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "log_prob = dist.log_prob(action)\n",
    "print(log_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.4189, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist.entropy().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.4189, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "entropy += dist.entropy().mean()\n",
    "print(entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[tensor([[-1.3377, -5.2006]], grad_fn=<SubBackward0>)]\n"
     ]
    }
   ],
   "source": [
    "print(log_probs)\n",
    "log_probs.append(log_prob)\n",
    "print(log_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[tensor([[-1.3420]], grad_fn=<AddmmBackward>)]\n"
     ]
    }
   ],
   "source": [
    "print(values)\n",
    "values.append(value)\n",
    "print(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[tensor([[-0.1151]])] torch.Size([1, 1])\n"
     ]
    }
   ],
   "source": [
    "print(rewards)\n",
    "rewards.append(torch.FloatTensor(reward).unsqueeze(1).to(device))\n",
    "print(rewards, rewards[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[tensor([[0.]])] torch.Size([1, 1])\n"
     ]
    }
   ],
   "source": [
    "print(masks)\n",
    "masks.append(torch.FloatTensor(mask).unsqueeze(1).to(device))\n",
    "print(masks, masks[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[tensor([[ 4.2920, -7.8069, 28.8004,  1.7667,  1.9213,  2.4526,  4.0000,  4.0000,\n",
      "          4.0000,  4.0000,  3.4205,  3.1118]])] torch.Size([1, 12])\n"
     ]
    }
   ],
   "source": [
    "print(states)\n",
    "states.append(state)\n",
    "print(states, states[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[tensor([[ 2.5054, -1.2700]])] torch.Size([1, 2])\n"
     ]
    }
   ],
   "source": [
    "print(actions)\n",
    "actions.append(action)\n",
    "print(actions, actions[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End one interation. Run loop for N times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 20\n",
    "for _ in range(1, num_steps):\n",
    "    state = torch.FloatTensor(state).to(device)\n",
    "    dist, value = model(state)\n",
    "\n",
    "    action = dist.sample()\n",
    "    env.set_actions(behaviorName, np.array(action.cpu()))\n",
    "    env.step()\n",
    "    \n",
    "    step_result = env.get_steps(behaviorName) \n",
    "    DecisionSteps = step_result[0] \n",
    "    TerminalSteps = step_result[1]\n",
    "    if(len(TerminalSteps) >0): # if we have terminal step, collect (s, r) from terminal step\n",
    "        next_state = TerminalSteps.obs[0]\n",
    "        reward = TerminalSteps.reward\n",
    "        mask=0\n",
    "    elif(len(DecisionSteps) >0): #otherwise collect (s, r) from decision steps\n",
    "        next_state = DecisionSteps.obs[0]\n",
    "        reward = DecisionSteps.reward\n",
    "        mask=1\n",
    "        \n",
    "    log_prob = dist.log_prob(action)\n",
    "    entropy += dist.entropy().mean()\n",
    "\n",
    "    log_probs.append(log_prob)\n",
    "    values.append(value)\n",
    "    rewards.append(torch.FloatTensor(reward).unsqueeze(1).to(device))\n",
    "    masks.append(torch.FloatTensor(mask).unsqueeze(1).to(device))\n",
    "\n",
    "    states.append(state)\n",
    "    actions.append(action)\n",
    "\n",
    "    state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 torch.Size([1, 2])\n",
      "20 torch.Size([1, 1])\n",
      "20 torch.Size([1, 1])\n",
      "20 torch.Size([1, 1])\n",
      "20 torch.Size([1, 12])\n",
      "20 torch.Size([1, 2])\n"
     ]
    }
   ],
   "source": [
    "print(len(log_probs), log_probs[0].shape)\n",
    "print(len(values), values[0].shape)\n",
    "print(len(rewards), rewards[0].shape)\n",
    "print(len(masks), masks[0].shape)\n",
    "print(len(states), states[0].shape)\n",
    "print(len(actions), actions[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_state = torch.FloatTensor(next_state).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, next_value = model(next_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# returns = compute_gae(next_value, rewards, masks, values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def compute_gae(...):\n",
    "\n",
    "    values = values + [next_value]\n",
    "    gae = 0\n",
    "    returns = []\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        delta = rewards[step] + gamma * values[step + 1] * masks[step] - values[step]\n",
    "        gae = delta + gamma * tau * masks[step] * gae\n",
    "        returns.insert(0, gae + values[step])\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma=0.99\n",
    "tau=0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 torch.Size([1, 1])\n",
      "21 torch.Size([1, 1])\n"
     ]
    }
   ],
   "source": [
    "values1 = values + [next_value]\n",
    "print(len(values), values[0].shape)\n",
    "print(len(values1), values[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "gae = 0\n",
    "returns = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0, "
     ]
    }
   ],
   "source": [
    "for step in reversed(range(len(rewards))):\n",
    "    print(step, end = \", \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for step in reversed(range(len(rewards))):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9543]], grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "step = 19\n",
    "delta = rewards[step] + gamma * values1[step + 1] * masks[step] - values1[step]\n",
    "print(delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9543]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "gae = delta + gamma * tau * masks[step] * gae\n",
    "print(gae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 torch.Size([1, 1])\n"
     ]
    }
   ],
   "source": [
    "returns.insert(0, gae + values1[step])\n",
    "print(len(returns), returns[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### finish one loop, now run for loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "values1 = values + [next_value]\n",
    "gae = 0\n",
    "returns = []\n",
    "for step in reversed(range(len(rewards))):\n",
    "    delta = rewards[step] + gamma * values1[step + 1] * masks[step] - values1[step]\n",
    "    gae = delta + gamma * tau * masks[step] * gae\n",
    "    returns.insert(0, gae + values1[step])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 torch.Size([1, 1])\n"
     ]
    }
   ],
   "source": [
    "print(len(returns), returns[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finish GAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 1])\n"
     ]
    }
   ],
   "source": [
    "returns = torch.cat(returns).detach()\n",
    "print(returns.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 torch.Size([1, 2])\n",
      "20 torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "print(len(log_probs), log_probs[0].shape)\n",
    "log_probs = torch.cat(log_probs).detach()\n",
    "print(len(log_probs), log_probs[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 torch.Size([1, 1])\n",
      "20 torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "print(len(values), values[0].shape)\n",
    "values=torch.cat(values).detach()\n",
    "print(len(values), values[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 torch.Size([1, 12])\n",
      "20 torch.Size([12])\n"
     ]
    }
   ],
   "source": [
    "print(len(states), states[0].shape)\n",
    "states = torch.cat(states)\n",
    "print(len(states), states[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 torch.Size([1, 2])\n",
      "20 torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "print(len(actions), actions[0].shape)\n",
    "actions = torch.cat(actions)\n",
    "print(len(actions), actions[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 1])\n"
     ]
    }
   ],
   "source": [
    "advantage = returns - values\n",
    "print(advantage.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enter  ppo_update(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def ppo_update(...):\n",
    "\n",
    "    for _ in range(ppo_epochs):\n",
    "        \n",
    "        for ... in ppo_iter(...):\n",
    "        \n",
    "            dist, value = model(state)\n",
    "            entropy = dist.entropy().mean()\n",
    "            new_log_probs = dist.log_prob(action)\n",
    "\n",
    "            ratio = (new_log_probs - old_log_probs).exp()\n",
    "            surr1 = ratio * advantage\n",
    "            surr2 = torch.clamp(...) * advantage\n",
    "\n",
    "            actor_loss  = - torch.min(surr1, surr2).mean()\n",
    "            critic_loss = (return_ - value).pow(2).mean()\n",
    "\n",
    "            loss = 0.5 * critic_loss + actor_loss - 0.001 * entropy\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def ppo_iter( ):\n",
    "    \n",
    "    batch_size = states.size(0)\n",
    "    for _ in range(batch_size // mini_batch_size):\n",
    "        rand_ids = np.random.randint(0, batch_size, mini_batch_size)\n",
    "        yield states[rand_ids, :], actions[rand_ids, :], log_probs[rand_ids, :], returns[rand_ids, :], advantage[rand_ids, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 12])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = states.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_batch_size  = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size // mini_batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*,*,*,*,"
     ]
    }
   ],
   "source": [
    "for _ in range(batch_size // mini_batch_size):\n",
    "    print(\"*\", end = \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3,  1, 15,  3,  0])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randint(0, batch_size, mini_batch_size) # 5 numbers between 0~39"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15 16  3 10 11], [ 5 18  5 14  6], [8 2 9 0 6], [ 6 15  4  1  3], [19  4  0 14  9], [ 6  6 12  1 13], [ 5 11  7  7 10], [ 2  5  3 19 13], [15 14 18 15  0], [ 7 11 11 13  3], "
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    rand_ids = np.random.randint(0, batch_size, mini_batch_size)\n",
    "    print(rand_ids, end = \", \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 12])\n",
      "[ 7 11 11 13  3]\n",
      "torch.Size([5, 12])\n"
     ]
    }
   ],
   "source": [
    "print(states.shape)\n",
    "print(rand_ids)\n",
    "print(states[rand_ids, :].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 2])\n",
      "torch.Size([5, 2])\n"
     ]
    }
   ],
   "source": [
    "print(actions.shape)\n",
    "print(actions[rand_ids, :].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 2])\n",
      "torch.Size([5, 2])\n"
     ]
    }
   ],
   "source": [
    "print(log_probs.shape)\n",
    "print(log_probs[rand_ids, :].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 1])\n",
      "torch.Size([5, 1])\n"
     ]
    }
   ],
   "source": [
    "print(returns.shape)\n",
    "print(returns[rand_ids, :].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 1])\n",
      "torch.Size([5, 1])\n"
     ]
    }
   ],
   "source": [
    "print(advantage.shape)\n",
    "print(advantage[rand_ids, :].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppo_iter(mini_batch_size, states, actions, log_probs, returns, advantage):\n",
    "    batch_size = states.size(0)\n",
    "    for _ in range(batch_size // mini_batch_size):\n",
    "        rand_ids = np.random.randint(0, batch_size, mini_batch_size)\n",
    "        yield states[rand_ids, :], actions[rand_ids, :], log_probs[rand_ids, :], returns[rand_ids, :], advantage[rand_ids, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "for state, action, old_log_probs, return_, advantage in ppo_iter(mini_batch_size, states, actions, log_probs, returns, advantage):\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal(loc: torch.Size([5, 2]), scale: torch.Size([5, 2]))\n",
      "torch.Size([5, 1])\n"
     ]
    }
   ],
   "source": [
    "dist, value = model(state)\n",
    "print(dist)\n",
    "print(value.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.4189, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "entropy = dist.entropy().mean()\n",
    "print(entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 2])\n"
     ]
    }
   ],
   "source": [
    "new_log_probs = dist.log_prob(action)\n",
    "print(new_log_probs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_param=0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = (new_log_probs - old_log_probs).exp()\n",
    "surr1 = ratio * advantage\n",
    "surr2 = torch.clamp(ratio, 1.0 - clip_param, 1.0 + clip_param) * advantage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_loss  = - torch.min(surr1, surr2).mean()\n",
    "critic_loss = (return_ - value).pow(2).mean()\n",
    "loss = 0.5 * critic_loss + actor_loss - 0.001 * entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test performance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "Max_test_frames = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_env():\n",
    "    env.reset()\n",
    "    step_result = env.get_steps(behaviorName) \n",
    "    DecisionSteps = step_result[0] \n",
    "    state = DecisionSteps.obs[0]        \n",
    "    total_reward = 0\n",
    "    frame_count = 0\n",
    "    stop_test = False\n",
    "    while (not stop_test):  \n",
    "        frame_count = frame_count + 1\n",
    "        if(frame_count > Max_test_frames):\n",
    "            stop_test = True\n",
    "        else:\n",
    "            state = torch.FloatTensor(state).to(device)\n",
    "            dist, _ = model(state)\n",
    "            action = dist.sample()\n",
    "            env.set_actions(behaviorName, np.array(action.cpu()))\n",
    "            env.step()\n",
    "            step_result = env.get_steps(behaviorName) \n",
    "            DecisionSteps = step_result[0]\n",
    "            TerminalSteps = step_result[1]\n",
    "            if(len(TerminalSteps) >0): # if reach terminal step, then stop\n",
    "                reward = DecisionSteps.reward\n",
    "                total_reward += reward\n",
    "                stop_test = True\n",
    "            else:\n",
    "                next_state = DecisionSteps.obs[0]\n",
    "                reward = DecisionSteps.reward\n",
    "                total_reward += reward\n",
    "                state = next_state\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-13.389808], dtype=float32)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-18.51781], dtype=float32), array([-30.503582], dtype=float32)]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[test_env() for _ in range(2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-15.021767"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([test_env() for _ in range(10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
